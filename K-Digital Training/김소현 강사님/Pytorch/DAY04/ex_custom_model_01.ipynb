{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용자 정의 모델 클래스\n",
    "- 부모 클래스 : nn.Module\n",
    "- 필수 오버라이딩 : \n",
    "    * \\__init__() : 모델 층 구성 즉, 설계\n",
    "    * forward() : 순방향 학습 진행 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 로딩\n",
    "import torch                                    # 텐서 관련 모듈\n",
    "import torch.nn as nn                           # 인공신경망 관련 모듈\n",
    "import torch.nn.functional as F                 # 인공신경망 관련 함수들 모듈 (손실함수, 활성화함수 등)\n",
    "import torch.optim as optim                     # 최적화 관련 모듈 (가중치, 절편 빠르게 찾아주는 알고리즘)\n",
    "from torchinfo import summary                   # 모델 구조 및 정보 관련 모듈\n",
    "from torchmetrics.regression import *           # 회귀 성능 지표 관련 모듈\n",
    "from torchmetrics.classification import *       # 분류 성능 지표 관련 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 고정\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 텐서 저장 및 실행 위치 설정\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [기본] 신경망 클래스 <hr>\n",
    "    * 입력층 - 입력 : 피쳐수 고정\n",
    "    * 출력층 - 출력 : 타겟수 고정\n",
    "    * 은닉층 - 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설계\n",
    "# 데이터셋 : 피쳐 4개, 타겟 1개, 회귀\n",
    "# 입 력 층 : 입력   4개    출력  20개   AF ReLU\n",
    "# 은 닉 층 : 입력  20개    출력 100개   AF ReLU\n",
    "# 출 력 층 : 입력 100개    출력   1개   AF X, (분류일때는 Sigmoid & Softmax)\n",
    "\n",
    "# 회귀일때는 출력층의 활성화함수가 필요 없이 그대로 값을 출력하면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수가 4개로 고정인 모델\n",
    "class MyModel(nn.Module):\n",
    "    # 인스턴스/객체 생성 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self):\n",
    "        # 부모 클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(4, 20) # W 4 + b 1 => 1P, 5*20 = 100개 변수\n",
    "        self.hidden_layer = nn.Linear(20, 100) # W 20 + b 1 => 1P, 21*100 = 2100개 변수\n",
    "        self.output_layer = nn.Linear(100, 1) # W 100 + b 1 => 1P, 101*1 = 101개 변수\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x)     # 1개 퍼셉트론 : y = x1W1 + x2W2 + x3W3 + x4W4+b (총 20개 만듦)\n",
    "        y = F.relu(y)               # 0 <= y ----> 죽은 relu ==> leakyReLU\n",
    "\n",
    "        y = self.hidden_layer(y)    # 1개 퍼셉트론 : y = x1W1+x2W2+...+x20W20+b (총 100개 만듦)\n",
    "        y = F.relu(y)\n",
    "\n",
    "        return self.output_layer(y) # 1개 퍼셉트론 : y = x1W1+x2W2+...+x100W100+b (총 1개 만듦)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수가 동적인 모델\n",
    "class MyModel2(nn.Module):\n",
    "    # 인스턴스/객체 생성 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self, in_features):\n",
    "        # 부모 클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(in_features, 20) # W 4 + b 1 => 1P, 5*20 = 100개 변수\n",
    "        self.hidden_layer = nn.Linear(20, 100) # W 20 + b 1 => 1P, 21*100 = 2100개 변수\n",
    "        self.output_layer = nn.Linear(100, 1) # W 100 + b 1 => 1P, 101*1 = 101개 변수\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x)     # 1개 퍼셉트론 : y = x1W1 + x2W2 + x3W3 + x4W4+b (총 20개 만듦)\n",
    "        y = F.relu(y)               # 0 <= y ----> 죽은 relu ==> leakyReLU\n",
    "\n",
    "        y = self.hidden_layer(y)    # 1개 퍼셉트론 : y = x1W1+x2W2+...+x20W20+b (총 100개 만듦)\n",
    "        y = F.relu(y)\n",
    "\n",
    "        return self.output_layer(y) # 1개 퍼셉트론 : y = x1W1+x2W2+...+x100W100+b (총 1개 만듦)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수, 은닉층 퍼셉트론 수 동적인 모델\n",
    "class MyModel3(nn.Module):\n",
    "    # 인스턴스/객체 생성 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self, in_features, in_out, h_out):\n",
    "        # 부모 클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(in_features, in_out)   # W 4 + b 1 => 1P, 5*20 = 100개 변수\n",
    "        self.hidden_layer = nn.Linear(in_out, h_out)        # W 20 + b 1 => 1P, 21*100 = 2100개 변수\n",
    "        self.output_layer = nn.Linear(h_out, 1)             # W 100 + b 1 => 1P, 101*1 = 101개 변수\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x)     # 1개 퍼셉트론 : y = x1W1 + x2W2 + x3W3 + x4W4+b (총 20개 만듦)\n",
    "        y = F.relu(y)               # 0 <= y ----> 죽은 relu ==> leakyReLU\n",
    "\n",
    "        y = self.hidden_layer(y)    # 1개 퍼셉트론 : y = x1W1+x2W2+...+x20W20+b (총 100개 만듦)\n",
    "        y = F.relu(y)\n",
    "\n",
    "        return self.output_layer(y) # 1개 퍼셉트론 : y = x1W1+x2W2+...+x100W100+b (총 1개 만듦)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은닉층의 개수가 동적인 모델\n",
    "class MyModel4(nn.Module):\n",
    "    # 인스턴스/객체 생성 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self, in_features, hidden_list):\n",
    "        # 부모 클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.hidden_list = hidden_list\n",
    "        self.hidden_layer_cnt = len(hidden_list)\n",
    "\n",
    "        self.input_layer = nn.Linear(in_features, hidden_list[0])   # W 4 + b 1 => 1P, 5*20 = 100개 변수\n",
    "\n",
    "        # hidden_layer 자리는 forward 쪽으로 뺌\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_list[self.hidden_layer_cnt-1], 1)    # W 100 + b 1 => 1P, 101*1 = 101개 변수\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x)     # 1개 퍼셉트론 : y = x1W1 + x2W2 + x3W3 + x4W4+b (총 20개 만듦)\n",
    "        y = F.relu(y)               # 0 <= y ----> 죽은 relu ==> leakyReLU\n",
    "\n",
    "        for i in range(self.hidden_layer_cnt):\n",
    "            self.hidden_layer = nn.Linear(self.hidden_list[i], self.hidden_list[i+1])\n",
    "            y = self.hidden_layer(y)\n",
    "            y = F.relu\n",
    "\n",
    "        return self.output_layer(y) # 1개 퍼셉트론 : y = x1W1+x2W2+...+x100W100+b (총 1개 만듦)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은닉층의 개수가 동적인 모델\n",
    "class MyModel4(nn.Module):\n",
    "    # 인스턴스/객체 생성 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    def __init__(self, in_features, hidden_list):\n",
    "        # 부모 클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식 클래스의 인스턴스 속성 설정\n",
    "        self.hidden_list = hidden_list\n",
    "        self.hidden_layer_cnt = len(hidden_list)-1\n",
    "\n",
    "        self.input_layer = nn.Linear(in_features, hidden_list[0])   # W 4 + b 1 => 1P, 5*20 = 100개 변수\n",
    "        # print(f'input layer : {self.input_layer}')\n",
    "\n",
    "        # 히든 레이어\n",
    "        self.hidden_layer_list = nn.ModuleList()\n",
    "        for i in range(self.hidden_layer_cnt):\n",
    "            self.hidden_layer_list.append(nn.Linear(self.hidden_list[i], self.hidden_list[i+1]))\n",
    "            # print(f'hidden layer{i+1} : {self.hidden_layer_list[i]}')\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_list[self.hidden_layer_cnt], 1)    # W 100 + b 1 => 1P, 101*1 = 101개 변수\n",
    "        # print(f'output layer : {self.output_layer}')\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수)\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x)     # 1개 퍼셉트론 : y = x1W1 + x2W2 + x3W3 + x4W4+b (총 20개 만듦)\n",
    "        y = F.relu(y)               # 0 <= y ----> 죽은 relu ==> leakyReLU\n",
    "\n",
    "        for i in self.hidden_layer_list:\n",
    "            y = i(y)\n",
    "            y = F.relu(y)\n",
    "\n",
    "        return self.output_layer(y) # 1개 퍼셉트론 : y = x1W1+x2W2+...+x100W100+b (총 1개 만듦)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스 생성\n",
    "m1 = MyModel2(in_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel4(\n",
       "  (input_layer): Linear(in_features=3, out_features=100, bias=True)\n",
       "  (hidden_layer_list): ModuleList(\n",
       "    (0): Linear(in_features=100, out_features=200, bias=True)\n",
       "    (1): Linear(in_features=200, out_features=300, bias=True)\n",
       "    (2): Linear(in_features=300, out_features=400, bias=True)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=400, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m4 = MyModel4(in_features=3, hidden_list=[100, 200, 300, 400])\n",
    "m4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('input_layer.weight', Parameter containing:\n",
      "tensor([[-0.5341,  0.4648, -0.5534],\n",
      "        [ 0.1510, -0.0716, -0.4107],\n",
      "        [-0.2082, -0.2223, -0.5268],\n",
      "        [-0.4919,  0.3113, -0.3583],\n",
      "        [-0.1807, -0.1970, -0.2683],\n",
      "        [-0.3693, -0.5101, -0.4672],\n",
      "        [-0.0986,  0.0320,  0.2292],\n",
      "        [-0.0751, -0.4333,  0.4379],\n",
      "        [ 0.0347,  0.1997, -0.4950],\n",
      "        [-0.5715, -0.0840,  0.0107],\n",
      "        [-0.0485,  0.1111, -0.5635],\n",
      "        [ 0.5011,  0.2536, -0.3822],\n",
      "        [ 0.2695, -0.3871, -0.5035],\n",
      "        [-0.3660, -0.0831, -0.4610],\n",
      "        [ 0.0271,  0.1010, -0.4564],\n",
      "        [-0.4992,  0.5024,  0.4691],\n",
      "        [ 0.4357,  0.4206,  0.3831],\n",
      "        [-0.2931, -0.4852, -0.1086],\n",
      "        [-0.3810,  0.4570,  0.3342],\n",
      "        [ 0.2351,  0.2713,  0.3153]], requires_grad=True))\n",
      "('input_layer.bias', Parameter containing:\n",
      "tensor([ 0.2114, -0.0610,  0.4379,  0.0813,  0.0749, -0.0525, -0.1201,  0.2234,\n",
      "        -0.1872,  0.4069,  0.1871,  0.0035,  0.5260, -0.0927,  0.5683, -0.2508,\n",
      "        -0.0403,  0.0070,  0.0828, -0.3273], requires_grad=True))\n",
      "('hidden_layer.weight', Parameter containing:\n",
      "tensor([[ 0.2163, -0.0168, -0.0373,  ..., -0.1492, -0.0926,  0.1422],\n",
      "        [ 0.0540, -0.2040, -0.2211,  ..., -0.1176,  0.0656,  0.0250],\n",
      "        [ 0.1985,  0.1019, -0.0761,  ...,  0.1325, -0.2114,  0.0206],\n",
      "        ...,\n",
      "        [ 0.0731,  0.0310,  0.2220,  ..., -0.0873, -0.1877,  0.0240],\n",
      "        [ 0.0101,  0.1189, -0.0317,  ...,  0.0677, -0.1278,  0.1032],\n",
      "        [-0.0819, -0.0663, -0.2098,  ..., -0.2145,  0.0577,  0.2065]],\n",
      "       requires_grad=True))\n",
      "('hidden_layer.bias', Parameter containing:\n",
      "tensor([-0.0824, -0.0288, -0.0132,  0.1877, -0.0552, -0.0435,  0.0125, -0.1653,\n",
      "         0.1102, -0.1506,  0.0782,  0.1171,  0.0048,  0.1489, -0.0838, -0.1239,\n",
      "         0.1187, -0.2100, -0.1531, -0.1191,  0.0757,  0.0349, -0.0402,  0.2044,\n",
      "        -0.0157, -0.1868, -0.0025, -0.1442, -0.0368,  0.0280,  0.1743,  0.1012,\n",
      "         0.1165,  0.0297, -0.1198, -0.2005,  0.2060, -0.0195,  0.1760, -0.0406,\n",
      "         0.0290, -0.0230, -0.1691,  0.0133, -0.0302, -0.1486, -0.0111,  0.1056,\n",
      "        -0.0929, -0.0252, -0.1832, -0.2140,  0.2205,  0.0967,  0.0535,  0.1993,\n",
      "        -0.1691, -0.0207,  0.1928,  0.1525, -0.2075,  0.1207, -0.0850,  0.0668,\n",
      "        -0.0317,  0.0442,  0.1899, -0.1441, -0.1118,  0.1512,  0.0577,  0.0486,\n",
      "        -0.2064,  0.1439, -0.0331,  0.0138,  0.1452, -0.0976,  0.1767, -0.0536,\n",
      "        -0.1842, -0.1103,  0.1314,  0.0063, -0.0613, -0.0133, -0.0930, -0.0657,\n",
      "         0.1997,  0.0844,  0.0386,  0.0913,  0.1662, -0.0035, -0.0225,  0.0750,\n",
      "        -0.2060, -0.0725, -0.2004,  0.0497], requires_grad=True))\n",
      "('output_layer.weight', Parameter containing:\n",
      "tensor([[ 0.0358, -0.0309, -0.0637,  0.0810,  0.0311, -0.0168, -0.0308, -0.0417,\n",
      "          0.0205,  0.0870, -0.0025,  0.0983, -0.0950,  0.0798,  0.0766,  0.0011,\n",
      "          0.0768, -0.0259, -0.0747, -0.0742, -0.0433, -0.0170,  0.0467, -0.0711,\n",
      "         -0.0854,  0.0246,  0.0186, -0.0365, -0.0984, -0.0282,  0.0877,  0.0265,\n",
      "          0.0844, -0.0722,  0.0917, -0.0156, -0.0554, -0.0962, -0.0083, -0.0364,\n",
      "         -0.0939,  0.0627, -0.0993, -0.0630,  0.0793, -0.0478, -0.0425, -0.0891,\n",
      "          0.0210,  0.0143,  0.0967,  0.0451, -0.0609, -0.0426,  0.0170, -0.0692,\n",
      "         -0.0631, -0.0030,  0.0930, -0.0405, -0.0684,  0.0261, -0.0856, -0.0626,\n",
      "         -0.0470, -0.0682, -0.0251,  0.0525,  0.0213, -0.0738, -0.0650,  0.0384,\n",
      "         -0.0938, -0.0669,  0.0833,  0.0373,  0.0673,  0.0396, -0.0943,  0.0393,\n",
      "         -0.0935,  0.0973,  0.0311,  0.0426,  0.0126,  0.0569,  0.0204, -0.0170,\n",
      "          0.0305,  0.0340,  0.0596, -0.0066,  0.0342,  0.0180, -0.0562,  0.0505,\n",
      "         -0.0402, -0.0761,  0.0966,  0.0073]], requires_grad=True))\n",
      "('output_layer.bias', Parameter containing:\n",
      "tensor([0.0202], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# 모델 파라미터 즉, W와 b 확인\n",
    "for m in m1.named_parameters(): print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling forward()\n",
      "tensor([[0.3860],\n",
      "        [0.5733]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 학습 진행 ==> 모델인스턴스명(데이터)\n",
    "# 임의의 데이터\n",
    "dataTS = torch.FloatTensor([[1, 3, 5], [2, 4, 6]])\n",
    "targetTS = torch.FloatTensor([[4],[5]])\n",
    "\n",
    "# 학습\n",
    "pred_y = m1(dataTS) # m1.forward(dataTS)로 해도 됨\n",
    "\n",
    "print(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
