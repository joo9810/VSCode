확률적 경사 하강법에서 loss = log로 지정하면 로지스틱 손실함수를 사용

케라스 모델
dense = keras.layers.Dense(10, activation = 'softmax', input_shape(784,)) <- 층생성
model = keras.Sequential(dense)
model.compile(loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')
model.fit(train_scaled, train_target, epochs = 5) <- 훈련
model.evaluate(val_scaled, val_target) <- 평가

2개의 층 (은닉층, 출력층) 참고로 입력층은 층으로 취급하지 않음

은닉층은 출력층이 10개의 뉴련이기 때문에 적어도 10개보단 많아야 한다.
출력층보다 작은 뉴런을 은닉층에 두면 정보가 많이 손실 됨

출력층에는 전형적으로 소프트맥스 함수를 사용 (다중분류) 이진분류일때는 시그모이드
은닉층에도 활성화 함수를 사용해야 한다. 은닉층의 활성화 함수가 없으면
은닉층의 수식과 출력층의 수식을 합쳐 하나의 수식으로 표현이 가능하기 때문에
2개의 층이 필요 없어진다. 따라서 은닉층은 둔다는 것은 반드시 활성화 함수를 거쳐서
비선형 함수로 데이터를 변형해서 만든다.

dense1 = keras.layers.Dense(100, activation = 'sigmoid', input_shape = (784,)) <- 은닉층
dense2 = keras.layers.Dense(10, activation = 'softmax') <- 출력층

model = keras.Sequential([dense1, dense2])

model.summary()
은닉층은 100개의 뉴런이므로 100개의 출력이 만들어지고
출력층은 10개의 뉴런이므로 10개의 출력이 만들어 짐

은닉층의 파라미터 갯수 784*100+100=78500
출력층의 파라미터 갯수 100*10+10=1010

층을 추가하는 다른 방법
model = keras.Sequential()
model.add(keras.layers.Dense(100, activation = 'sigmoid', input_shape = (784,)))
model.add(keras.layers.Dense(10, activation = 'softmax'))

렐루 함수와 Flatten 층
model = keras.Sequential()
model.add(keras.layers.Flatten(input_shape = (28, 28)))
model.add(keras.layers.Dense(100, activation = 'relu'))
model.add(keras.layers.Dense(10, activation = 'softmax'))
플래튼 층은 편의를 위해 만드는 층 (플래튼 층은 학습되는 파라미터(가중치)가 없음)
실제로 의미있는 작업을 하는 건 아니고 편의를 위해서 만드는 층
플래튼 층은 28*28의 2차원 이미지를 1차원 784 배열로 펼치는 작업을 대신 해 줌
따라서 플래튼층의 출력크기를 보면 784라고 되어 있다.

필요한 전처리 과정을 따로 별도의 파이썬 코드로 작성하지 않고 가능하면
모델 안에 포함시키는 것이 유지 보수의 측면에서 좋다

옵티마이저
model.compile(optimizer = 'sgd', loss = sparse_categorical_crossentropy', metrics = 'accuracy')
# optimizer = 'sgd': 확률적 경사 하강법 (기본적으로 32개씩 미니배치)

최적화 옵티마이저가 여러개 있고 가장 기본적인게 sgd이다.
sgd에서 조금 더 발전 된게 모멘텀, 모멘텀에서 좀 더 발전 된게 네스테로프 모멘텀
모멘텀과 RMSprop에 영향을 받은게 Adam

적응적 학습률 옵티마이저: RMSprop, Adam, Adagrad
적응적 학습률 옵티마이저는 학습 초반에는 경사를 빨리 내려가고 학습 후반에는
경사를 천천히 내려가는 등의 조절을 함 (학습률이 모델을 점차 학습함에 따라 변화)

SGD 옵티마이저
sgd = keras.optimizers.SGD() <- SGD 옵티마이저 객체를 만든 다음 optimizer 매개변수에 전달
model.compile(optimizer = sgd, loss = sparse_categorical_crossentropy', metrics = 'accuracy')

sgd = keras.optimizers.SGD(learning_rate = 0.1)
sgd = keras.optimizers.SGD(momentum = 0.9, nesterov = True)

Adam 옵티마이저
model = keras.Sequential()
model.add(keras.layers.Flatten(input_shape = (28, 28)))
model.add(keras.layers.Dense(100, activation = 'relu'))
model.add(keras.layers.Dense(10, activation = 'softmax'))

model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = accuracy')
model.fit(train_scaled, train_target, epochs = 5)

model.evaluate(val_scaled, val_target)